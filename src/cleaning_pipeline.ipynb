{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf76eeb",
   "metadata": {},
   "source": [
    "# Obtención y Limpieza de los datos\n",
    "\n",
    "Proyecto 1 - Data Science\n",
    "\n",
    "Edwin Ortega 22305 - Esteban Zambrano 22119 - Diego García 22404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd40593",
   "metadata": {},
   "source": [
    "### Configuración e importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d19392ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from xlrd import XLRDError\n",
    "\n",
    "# Paths\n",
    "DATA_RAW = Path(\"../data/raw_data\")\n",
    "DATA_INTERIM = Path(\"../data/provisional\")\n",
    "DATA_PROCESSED = Path(\"../data/procesada\")\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archivos esperados\n",
    "DEPARTAMENTOS = [\n",
    "    \"AltaVerapaz\",\"BajaVerapaz\",\"Chimaltenango\",\"Chiquimula\",\"CiudadCapital\",\n",
    "    \"ElProgreso\",\"Escuintla\",\"Guatemala\",\"Huehuetenango\",\"Izabal\",\"Jalapa\",\n",
    "    \"Jutiapa\",\"Peten\",\"Quetzaltenango\",\"Quiche\",\"Retalhuleu\",\"Sacatepequez\",\n",
    "    \"SanMarcos\",\"SantaRosa\",\"Solola\",\"Suchitepequez\",\"Totonicapan\",\"Zacapa\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5aa46",
   "metadata": {},
   "source": [
    "### Consolidacion crudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape crudo concatenado: (6599, 18)\n",
      "Columnas: ['area', 'codigo', 'departamental', 'departamento', 'departamento_origen', 'direccion', 'director', 'distrito', 'establecimiento', 'jornada', 'modalidad', 'municipio', 'nivel', 'plan', 'sector', 'status', 'supervisor', 'telefono']\n",
      "Conteo por departamento:\n",
      "departamento_origen\n",
      "Guatemala         1038\n",
      "CiudadCapital      866\n",
      "SanMarcos          432\n",
      "Escuintla          393\n",
      "Quetzaltenango     365\n",
      "Chimaltenango      304\n",
      "Jutiapa            296\n",
      "Suchitepequez      296\n",
      "Huehuetenango      295\n",
      "AltaVerapaz        294\n",
      "Izabal             273\n",
      "Retalhuleu         272\n",
      "Peten              270\n",
      "Sacatepequez       208\n",
      "Quiche             184\n",
      "Chiquimula         136\n",
      "SantaRosa          133\n",
      "Jalapa             121\n",
      "Solola             111\n",
      "ElProgreso          97\n",
      "BajaVerapaz         94\n",
      "Zacapa              70\n",
      "Totonicapan         51\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Columnas esperadas (en MAYÚSCULAS y sin acentos)\n",
    "EXPECTED = {\n",
    "    \"CODIGO\",\"DISTRITO\",\"DEPARTAMENTO\",\"MUNICIPIO\",\"ESTABLECIMIENTO\",\"DIRECCION\",\n",
    "    \"TELEFONO\",\"SUPERVISOR\",\"DIRECTOR\",\"NIVEL\",\"SECTOR\",\"AREA\",\"STATUS\",\n",
    "    \"MODALIDAD\",\"JORNADA\",\"PLAN\",\"DEPARTAMENTAL\"\n",
    "}\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplana encabezados raros, quita 'Unnamed', pasa a string+UPPER+sin acentos.\n",
    "       Si la primera fila parece ser header real, la usa como encabezados\"\"\"\n",
    "    # Aplastar MultiIndex si hay\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\" \".join(map(str, t)).strip() for t in df.columns.values]\n",
    "\n",
    "    # Normalizar encabezados actuales\n",
    "    df.columns = [unidecode(str(c)).upper().strip() for c in df.columns]\n",
    "    # Quitar columnas basura\n",
    "    df = df.loc[:, ~df.columns.str.startswith(\"UNNAMED\")]\n",
    "\n",
    "    if len(df) > 0:\n",
    "        first = [unidecode(str(x)).upper().strip() for x in df.iloc[0].tolist()]\n",
    "        if set(EXPECTED).issubset(set(first)):\n",
    "            df = df.iloc[1:].copy()\n",
    "            df.columns = first\n",
    "    return df\n",
    "\n",
    "def _pick_html_table(fp: str) -> pd.DataFrame:\n",
    "    \"\"\"Lee todas las tablas HTML y escoge la mejor según coincidencia con EXPECTED y tamaño\"\"\"\n",
    "    tables = pd.read_html(fp)\n",
    "    best = None\n",
    "    best_score = (-1, -1)\n",
    "\n",
    "    for t in tables:\n",
    "        t = t.astype(str)\n",
    "        t = _normalize_columns(t)\n",
    "        cols = set(t.columns)\n",
    "        matches = len(cols & EXPECTED)\n",
    "        score = (matches, len(t))\n",
    "        if score > best_score:\n",
    "            best, best_score = t, score\n",
    "\n",
    "    if best is None:\n",
    "        raise ValueError(f\"{fp} no contiene tablas HTML aprovechables.\")\n",
    "    return best\n",
    "\n",
    "def read_one_excel(dep: str) -> pd.DataFrame:\n",
    "    \"\"\"Lee un archivo por departamento desde DATA_RAW\"\"\"\n",
    "    for ext in (\".xlsx\", \".xls\"):\n",
    "        fp = DATA_RAW / f\"{dep}{ext}\"\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if ext == \".xlsx\":\n",
    "                df = pd.read_excel(fp, dtype=str, engine=\"openpyxl\")\n",
    "            else:  # \".xls\"\n",
    "                try:\n",
    "                    df = pd.read_excel(fp, dtype=str, engine=\"xlrd\")\n",
    "                except (ValueError, XLRDError):\n",
    "                    # .xls que en realidad es HTML\n",
    "                    df = _pick_html_table(str(fp))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error leyendo {fp}: {e}\")\n",
    "\n",
    "        # Normalizar encabezados y limpiar ruidos típicos\n",
    "        df = _normalize_columns(df)\n",
    "\n",
    "        # Marcar strings vacíos como NA y eliminar filas totalmente vacías\n",
    "        df = df.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "        df = df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "        # Convertir a Strings para evitar 'nan' literales\n",
    "        for c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\")\n",
    "\n",
    "        df[\"DEPARTAMENTO_ORIGEN\"] = dep\n",
    "        return df\n",
    "\n",
    "    raise FileNotFoundError(f\"No se encontró {dep}.xlsx ni {dep}.xls en {DATA_RAW}\")\n",
    "\n",
    "# LECTURA de todos los departamentos\n",
    "dfs = [read_one_excel(dep) for dep in DEPARTAMENTOS]\n",
    "raw = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "# Normalizar por si acaso tras el concat\n",
    "raw.columns = [unidecode(str(c)).upper().strip() for c in raw.columns]\n",
    "\n",
    "# Renombrado estándar a minúsculas finales\n",
    "col_map = {\n",
    "    \"CODIGO\":\"codigo\",\"DISTRITO\":\"distrito\",\"DEPARTAMENTO\":\"departamento\",\n",
    "    \"MUNICIPIO\":\"municipio\",\"ESTABLECIMIENTO\":\"establecimiento\",\"DIRECCION\":\"direccion\",\n",
    "    \"TELEFONO\":\"telefono\",\"SUPERVISOR\":\"supervisor\",\"DIRECTOR\":\"director\",\n",
    "    \"NIVEL\":\"nivel\",\"SECTOR\":\"sector\",\"AREA\":\"area\",\"STATUS\":\"status\",\n",
    "    \"MODALIDAD\":\"modalidad\",\"JORNADA\":\"jornada\",\"PLAN\":\"plan\",\"DEPARTAMENTAL\":\"departamental\",\n",
    "    \"DEPARTAMENTO_ORIGEN\":\"departamento_origen\"\n",
    "}\n",
    "raw = raw.rename(columns=col_map)\n",
    "\n",
    "# Normaliza vacíos y 'nan'/'None' en TODAS las columnas a NA reales\n",
    "raw = raw.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "raw = raw.replace(r'^\\s*(nan|none|null)\\s*$', pd.NA, regex=True)\n",
    "\n",
    "# Quita filas totalmente vacías\n",
    "raw = raw.dropna(how=\"all\")\n",
    "\n",
    "# Exige que haya al menos 3 campos NO nulos (ignorando 'departamento_origen')\n",
    "core_cols = [c for c in raw.columns if c != \"departamento_origen\"]\n",
    "raw = raw[ raw[core_cols].notna().sum(axis=1) >= 3 ]\n",
    "\n",
    "# Si existen, exige además 'codigo' y 'establecimiento'\n",
    "if {\"codigo\",\"establecimiento\"}.issubset(raw.columns):\n",
    "    raw = raw.dropna(subset=[\"codigo\",\"establecimiento\"], how=\"any\")\n",
    "\n",
    "# Guarda CSV's\n",
    "raw.to_csv(DATA_INTERIM / \"establecimientos_diversificado_raw_concat.csv\",\n",
    "           index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Shape crudo concatenado:\", raw.shape)\n",
    "print(\"Columnas:\", sorted(raw.columns))\n",
    "print(\"Conteo por departamento:\")\n",
    "print(raw[\"departamento_origen\"].value_counts(dropna=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
